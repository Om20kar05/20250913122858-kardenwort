{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Финальный код, основанный на официальной документации ---\n",
    "\n",
    "# 1. Импортируем нужные функции из установленной библиотеки\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Библиотека 'german-compound-splitter' успешно импортирована.\")\n",
    "\n",
    "# 2. Указываем имя файла словаря\n",
    "#    (предполагается, что он лежит в той же папке, что и этот скрипт)\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "# 3. Проверяем, существует ли файл словаря\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nОШИБКА: Файл словаря '{dictionary_file}' не найден!\")\n",
    "    print(\"Пожалуйста, скачайте его и поместите в ту же папку, что и этот ноутбук.\")\n",
    "else:\n",
    "    print(f\"\\nНайден файл словаря: '{dictionary_file}'. Загружаем...\")\n",
    "    # Загружаем словарь в специальную структуру данных (автомат Ахо-Корасик)\n",
    "    # Это нужно сделать только один раз.\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Словарь успешно загружен.\")\n",
    "\n",
    "        # 4. Наше слово для разбора\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "\n",
    "        # 5. Вызываем функцию dissect для разбора слова\n",
    "        #    Используем `make_singular=True`, чтобы привести части к единственному числу\n",
    "        dissection = comp_split.dissect(compound_word, ahocs, make_singular=True)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Анализируем слово: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        # Результат - это кортеж из двух списков. Нас интересует второй.\n",
    "        # merge_fractions объединяет мелкие части для лучшего результата.\n",
    "        final_components = comp_split.merge_fractions(dissection)\n",
    "        \n",
    "        print(\"✅✅✅ ПОБЕДА! Слово успешно разделено на компоненты:\")\n",
    "        for i, part in enumerate(final_components, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nПроизошла ошибка при чтении словаря: {e}\")\n",
    "        print(\"Возможно, файл имеет неверную кодировку. Он должен быть в UTF-8.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a9d739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Библиотека 'german-compound-splitter' успешно импортирована.\n",
      "\n",
      "Найден файл словаря: 'german.dic'. Загружаем...\n",
      "Loading data file - german.dic\n",
      "Словарь успешно загружен.\n",
      "Dissect compound:  Donaudampfschifffahrtsgesellschaftskapitän\n",
      "\n",
      "=================================================================\n",
      "Анализируем слово: 'Donaudampfschifffahrtsgesellschaftskapitän'\n",
      "=================================================================\n",
      "✅✅✅ ПОБЕДА! Слово успешно разделено на компоненты:\n",
      "  1. Donau\n",
      "  2. Dampf\n",
      "  3. Schifffahrt\n",
      "  4. Gesellschafts\n",
      "  5. Kapitän\n"
     ]
    }
   ],
   "source": [
    "# --- Финальный код, основанный на официальной документации ---\n",
    "\n",
    "# 1. Импортируем нужные функции из установленной библиотеки\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Библиотека 'german-compound-splitter' успешно импортирована.\")\n",
    "\n",
    "# 2. Указываем имя файла словаря\n",
    "#    (предполагается, что он лежит в той же папке, что и этот скрипт)\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "# 3. Проверяем, существует ли файл словаря\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nОШИБКА: Файл словаря '{dictionary_file}' не найден!\")\n",
    "    print(\"Пожалуйста, скачайте его и поместите в ту же папку, что и этот ноутбук.\")\n",
    "else:\n",
    "    print(f\"\\nНайден файл словаря: '{dictionary_file}'. Загружаем...\")\n",
    "    # Загружаем словарь в специальную структуру данных (автомат Ахо-Корасик)\n",
    "    # Это нужно сделать только один раз.\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Словарь успешно загружен.\")\n",
    "\n",
    "        # 4. Наше слово для разбора\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "\n",
    "        # 5. Вызываем функцию dissect для разбора слова\n",
    "        #    Используем `make_singular=True`, чтобы привести части к единственному числу\n",
    "        dissection = comp_split.dissect(compound_word, ahocs, make_singular=False)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Анализируем слово: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        # Результат - это кортеж из двух списков. Нас интересует второй.\n",
    "        # merge_fractions объединяет мелкие части для лучшего результата.\n",
    "        final_components = comp_split.merge_fractions(dissection)\n",
    "        \n",
    "        print(\"✅✅✅ ПОБЕДА! Слово успешно разделено на компоненты:\")\n",
    "        for i, part in enumerate(final_components, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nПроизошла ошибка при чтении словаря: {e}\")\n",
    "        print(\"Возможно, файл имеет неверную кодировку. Он должен быть в UTF-8.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fd3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Финальный код с рекурсивной функцией для максимального разбора ---\n",
    "\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Библиотека 'german-compound-splitter' успешно импортирована.\")\n",
    "\n",
    "# --- НАЧАЛО НОВОГО КОДА: РЕКУРСИВНАЯ ФУНКЦИЯ ---\n",
    "\n",
    "def razobrat_rekursivno(slovo, ahocs):\n",
    "    \"\"\"\n",
    "    Разбирает слово и рекурсивно пытается разобрать каждую его часть.\n",
    "    \"\"\"\n",
    "    # 1. Выполняем стандартный разбор слова\n",
    "    dissection = comp_split.dissect(slovo, ahocs, make_singular=True)\n",
    "    chasti = comp_split.merge_fractions(dissection)\n",
    "\n",
    "    # 2. БАЗОВЫЙ СЛУЧАЙ РЕКУРСИИ:\n",
    "    # Если слово не удалось разбить (результат - это само слово),\n",
    "    # то возвращаем его как есть в виде списка.\n",
    "    if len(chasti) <= 1 and chasti[0].lower() == slovo.lower():\n",
    "        return chasti\n",
    "\n",
    "    # 3. РЕКУРСИВНЫЙ ШАГ:\n",
    "    # Если слово было разбито на части, то для каждой части\n",
    "    # снова вызываем эту же функцию.\n",
    "    else:\n",
    "        itogovyy_razbor = []\n",
    "        for chast in chasti:\n",
    "            # Вызываем функцию для каждой полученной части\n",
    "            pod_chasti = razobrat_rekursivno(chast, ahocs)\n",
    "            # Добавляем результат (который является списком) в наш итоговый список\n",
    "            itogovyy_razbor.extend(pod_chasti)\n",
    "        return itogovyy_razbor\n",
    "\n",
    "# --- КОНЕЦ НОВОГО КОДА ---\n",
    "\n",
    "\n",
    "# --- ОСНОВНАЯ ЧАСТЬ (остается почти без изменений) ---\n",
    "\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nОШИБКА: Файл словаря '{dictionary_file}' не найден!\")\n",
    "else:\n",
    "    print(f\"\\nНайден файл словаря: '{dictionary_file}'. Загружаем...\")\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Словарь успешно загружен.\")\n",
    "\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "        \n",
    "        # --- ИЗМЕНЕНИЕ: ВЫЗЫВАЕМ НАШУ НОВУЮ РЕКУРСИВНУЮ ФУНКЦИЮ ---\n",
    "        final_components = razobrat_rekursivno(compound_word, ahocs)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Анализируем слово: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        print(\"✅✅✅ ПОБЕДА! Максимально детальный (рекурсивный) разбор:\")\n",
    "        for i, part in enumerate(final_components, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nПроизошла ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Финальный код с ИСПРАВЛЕННОЙ рекурсивной функцией ---\n",
    "\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Библиотека 'german-compound-splitter' успешно импортирована.\")\n",
    "\n",
    "# --- НАЧАЛО ИСПРАВЛЕННОГО КОДА ---\n",
    "\n",
    "def razobrat_rekursivno(slovo, ahocs, level=0):\n",
    "    \"\"\"\n",
    "    Разбирает слово и рекурсивно пытается разобрать каждую его часть.\n",
    "    Level - для красивого отступа в отладочных сообщениях.\n",
    "    \"\"\"\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}--> Анализирую: '{slovo}'\")\n",
    "\n",
    "    # 1. Выполняем стандартный разбор слова\n",
    "    dissection = comp_split.dissect(slovo, ahocs, make_singular=True)\n",
    "    chasti = comp_split.merge_fractions(dissection)\n",
    "    \n",
    "    print(f\"{indent}----> Результат dissect: {chasti}\")\n",
    "\n",
    "    # 2. НОВЫЙ БАЗОВЫЙ СЛУЧАЙ РЕКУРСИИ:\n",
    "    # Рекурсия останавливается ТОЛЬКО, если слово состоит из ОДНОЙ части\n",
    "    # И при этом эта часть КОРОЧЕ 4 символов (чтобы не делить 'Kap' на 'K' и 'ap').\n",
    "    # Это предотвращает бесконечные циклы и бессмысленный разбор.\n",
    "    if len(chasti) == 1 and len(chasti[0]) < 5:\n",
    "         print(f\"{indent}------> Базовый случай: слово короткое, не делим.\")\n",
    "         return chasti\n",
    "\n",
    "    # 3. ИСПРАВЛЕННЫЙ РЕКУРСИВНЫЙ ШАГ:\n",
    "    # Если dissect вернул само слово (т.е. поленился), мы должны попробовать\n",
    "    # разбить его вручную.\n",
    "    if len(chasti) == 1 and chasti[0].lower() == slovo.lower():\n",
    "        print(f\"{indent}------> Dissect 'схалтурил'. Пробуем найти первую часть вручную...\")\n",
    "        # Ищем первую же часть слова, которая есть в словаре\n",
    "        pervaya_chast = comp_split.find_first_part(slovo, ahocs)\n",
    "        if pervaya_chast and pervaya_chast != slovo:\n",
    "            ostatok = slovo[len(pervaya_chast):]\n",
    "            print(f\"{indent}------> Вручную нашли: '{pervaya_chast}' + '{ostatok}'. Уходим в рекурсию для них.\")\n",
    "            # Если нашли, создаем список из двух частей и обрабатываем их рекурсивно\n",
    "            chasti = [pervaya_chast, ostatok]\n",
    "        else:\n",
    "            # Если даже вручную не нашли, то это точно базовый случай\n",
    "            return [slovo]\n",
    "\n",
    "    itogovyy_razbor = []\n",
    "    for chast in chasti:\n",
    "        # Проверяем, чтобы не уйти в бесконечную рекурсию, если часть равна исходному слову\n",
    "        if chast.lower() == slovo.lower():\n",
    "            itogovyy_razbor.append(chast)\n",
    "        else:\n",
    "            pod_chasti = razobrat_rekursivno(chast, ahocs, level + 1)\n",
    "            itogovyy_razbor.extend(pod_chasti)\n",
    "    \n",
    "    return itogovyy_razbor\n",
    "\n",
    "# --- КОНЕЦ ИСПРАВЛЕННОГО КОДА ---\n",
    "\n",
    "\n",
    "# --- ОСНОВНАЯ ЧАСТЬ (без изменений) ---\n",
    "\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nОШИБКА: Файл словаря '{dictionary_file}' не найден!\")\n",
    "else:\n",
    "    print(f\"\\nНайден файл словаря: '{dictionary_file}'. Загружаем...\")\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Словарь успешно загружен.\")\n",
    "\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "        \n",
    "        final_components = razobrat_rekursivno(compound_word, ahocs)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Анализируем слово: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        print(\"✅✅✅ ПОБЕДА! Максимально детальный (рекурсивный) разбор:\")\n",
    "        for i, part in enumerate(final_components, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nПроизошла ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final, professional version with English identifiers and comments ---\n",
    "\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Library 'german-compound-splitter' imported successfully.\")\n",
    "\n",
    "# --- START: HELPER FUNCTIONS ---\n",
    "\n",
    "def manual_split(word, ahocs):\n",
    "    \"\"\"\n",
    "    Manually tries to find the first possible split combination for a word.\n",
    "    It searches for the longest possible first part that exists in the dictionary,\n",
    "    for which the remainder also exists in the dictionary.\n",
    "    \"\"\"\n",
    "    # Iterate from the longest possible first part to the shortest\n",
    "    for i in range(len(word) - 1, 1, -1):\n",
    "        first_part = word[:i]\n",
    "        remainder = word[i:]\n",
    "        \n",
    "        # Check if BOTH parts exist in our dictionary automaton\n",
    "        if ahocs.exists(first_part) and ahocs.exists(remainder):\n",
    "            # If a valid combination is found, return it and exit\n",
    "            return [first_part, remainder]\n",
    "            \n",
    "    # If no valid two-part split is found, return the original word\n",
    "    return [word]\n",
    "\n",
    "def recursive_split(word, ahocs, level=0):\n",
    "    \"\"\"\n",
    "    Dissects a word and recursively tries to dissect each of its parts.\n",
    "    'level' is used for pretty-printing the debug trace.\n",
    "    \"\"\"\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}--> Analyzing: '{word}'\")\n",
    "    \n",
    "    # 1. Perform the standard dissection\n",
    "    dissection = comp_split.dissect(word, ahocs, make_singular=True)\n",
    "    parts = comp_split.merge_fractions(dissection)\n",
    "    print(f\"{indent}----> Result from dissect: {parts}\")\n",
    "\n",
    "    # 2. Check if the 'dissect' function was \"lazy\" (returned the word itself)\n",
    "    if len(parts) == 1 and parts[0].lower() == word.lower():\n",
    "        print(f\"{indent}------> 'dissect' returned the whole word. Calling manual split...\")\n",
    "        # If so, call our own, more persistent split function\n",
    "        parts = manual_split(word, ahocs)\n",
    "        print(f\"{indent}------> Result from manual split: {parts}\")\n",
    "\n",
    "    # 3. Base Case: If the word still couldn't be split, stop the recursion for this branch\n",
    "    if len(parts) == 1 and parts[0].lower() == word.lower():\n",
    "        return parts\n",
    "\n",
    "    # 4. Recursive Step: For each of the new parts, call this function again\n",
    "    final_split_parts = []\n",
    "    for part in parts:\n",
    "        sub_parts = recursive_split(part, ahocs, level + 1)\n",
    "        final_split_parts.extend(sub_parts)\n",
    "    \n",
    "    return final_split_parts\n",
    "\n",
    "# --- END: HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION BLOCK ---\n",
    "\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nERROR: Dictionary file '{dictionary_file}' not found!\")\n",
    "    print(\"Please download it and place it in the same folder as this notebook.\")\n",
    "else:\n",
    "    print(f\"\\nDictionary file found: '{dictionary_file}'. Loading...\")\n",
    "    try:\n",
    "        # Load the dictionary into the Aho-Corasick automaton structure\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Dictionary loaded successfully.\")\n",
    "\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "        \n",
    "        # Call our main recursive function to get the most detailed split\n",
    "        final_components = recursive_split(compound_word, ahocs)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Analyzing word: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        print(\"✅✅✅ SUCCESS! Deep (recursive) split results:\")\n",
    "        for i, part in enumerate(final_components, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274200d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final, corrected version with the case-sensitivity fix in manual_split ---\n",
    "\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Library 'german-compound-splitter' imported successfully.\")\n",
    "\n",
    "# --- START: CORRECTED HELPER FUNCTIONS ---\n",
    "\n",
    "def manual_split(word, ahocs):\n",
    "    \"\"\"\n",
    "    Manually tries to find the first possible split combination for a word.\n",
    "    It searches for the longest possible first part that exists in the dictionary,\n",
    "    for which the remainder also exists in the dictionary.\n",
    "    THIS VERSION FIXES THE CASE-SENSITIVITY BUG.\n",
    "    \"\"\"\n",
    "    for i in range(len(word) - 1, 1, -1):\n",
    "        first_part = word[:i]\n",
    "        remainder = word[i:]\n",
    "        \n",
    "        # THE FIX IS HERE: We must check for the capitalized version of the parts,\n",
    "        # because the dictionary contains capitalized nouns.\n",
    "        if ahocs.exists(first_part.capitalize()) and ahocs.exists(remainder.capitalize()):\n",
    "            # Return the original parts to preserve their case.\n",
    "            return [first_part, remainder]\n",
    "            \n",
    "    return [word]\n",
    "\n",
    "def recursive_split(word, ahocs, level=0):\n",
    "    \"\"\"\n",
    "    Dissects a word and recursively tries to dissect each of its parts.\n",
    "    \"\"\"\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}--> Analyzing: '{word}'\")\n",
    "    \n",
    "    dissection = comp_split.dissect(word, ahocs, make_singular=True)\n",
    "    parts = comp_split.merge_fractions(dissection)\n",
    "    print(f\"{indent}----> Result from dissect: {parts}\")\n",
    "\n",
    "    if len(parts) == 1 and parts[0].lower() == word.lower():\n",
    "        print(f\"{indent}------> 'dissect' returned the whole word. Calling manual split...\")\n",
    "        parts = manual_split(word, ahocs)\n",
    "        print(f\"{indent}------> Result from manual split: {parts}\")\n",
    "\n",
    "    if len(parts) == 1 and parts[0].lower() == word.lower():\n",
    "        return parts\n",
    "\n",
    "    final_split_parts = []\n",
    "    for part in parts:\n",
    "        sub_parts = recursive_split(part, ahocs, level + 1)\n",
    "        final_split_parts.extend(sub_parts)\n",
    "    \n",
    "    return final_split_parts\n",
    "\n",
    "# --- END: CORRECTED HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION BLOCK ---\n",
    "\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nERROR: Dictionary file '{dictionary_file}' not found!\")\n",
    "else:\n",
    "    print(f\"\\nDictionary file found: '{dictionary_file}'. Loading...\")\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Dictionary loaded successfully.\")\n",
    "\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "        \n",
    "        final_components = recursive_split(compound_word, ahocs)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Analyzing word: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        print(\"✅✅✅ SUCCESS! Deep (recursive) split results:\")\n",
    "        for i, part in enumerate(final_components, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Кардинально переделанный код с новым, надежным алгоритмом ---\n",
    "\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Library 'german-compound-splitter' imported successfully.\")\n",
    "\n",
    "# --- НОВАЯ, ПРОСТАЯ И НАДЕЖНАЯ ФУНКЦИЯ ---\n",
    "def deep_split(compound_word, ahocs):\n",
    "    \"\"\"\n",
    "    Выполняет глубокий, итеративный разбор слова до тех пор,\n",
    "    пока все части не станут атомарными.\n",
    "    \"\"\"\n",
    "    # Создаем \"очередь\" слов, которые нужно проверить. Начинаем с одного слова.\n",
    "    words_to_process = [compound_word]\n",
    "    \n",
    "    # Здесь будут храниться финальные, самые мелкие части.\n",
    "    final_parts = []\n",
    "    \n",
    "    # Цикл будет работать, пока в очереди есть слова для обработки.\n",
    "    while words_to_process:\n",
    "        # Берем первое слово из очереди.\n",
    "        current_word = words_to_process.pop(0)\n",
    "        \n",
    "        # Пытаемся разбить его с помощью библиотеки.\n",
    "        # Это наш основной и единственный инструмент.\n",
    "        parts = comp_split.merge_fractions(\n",
    "            comp_split.dissect(current_word, ahocs, make_singular=True)\n",
    "        )\n",
    "        \n",
    "        # ПРОВЕРКА: Удалось ли разбить слово?\n",
    "        # Если результат - это НЕ одно слово, равное исходному, значит, разбор удался.\n",
    "        if len(parts) > 1 or (len(parts) == 1 and parts[0].lower() != current_word.lower()):\n",
    "            # Если разбор удался, мы добавляем новые, более мелкие части\n",
    "            # В НАЧАЛО очереди, чтобы обработать их следующими.\n",
    "            print(f\"  -> '{current_word}' ==> {parts}. Adding to queue.\")\n",
    "            words_to_process = parts + words_to_process\n",
    "        else:\n",
    "            # Если разбор НЕ удался (слово атомарно),\n",
    "            # мы добавляем его в наш финальный результат.\n",
    "            print(f\"  -> '{current_word}' is atomic. Adding to final list.\")\n",
    "            final_parts.append(current_word)\n",
    "            \n",
    "    return final_parts\n",
    "\n",
    "# --- ОСНОВНАЯ ЧАСТЬ ---\n",
    "\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nERROR: Dictionary file '{dictionary_file}' not found!\")\n",
    "else:\n",
    "    print(f\"\\nDictionary file found: '{dictionary_file}'. Loading...\")\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Dictionary loaded successfully.\")\n",
    "\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "        \n",
    "        print(\"\\nStarting deep split process...\")\n",
    "        final_components = deep_split(compound_word, ahocs)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Analyzing word: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        print(\"✅✅✅ SUCCESS! Deep (iterative) split results:\")\n",
    "        for i, part in enumerate(final_components, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Кардинально переделанный код с новым, надежным алгоритмом ---\n",
    "\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Library 'german-compound-splitter' imported successfully.\")\n",
    "\n",
    "# --- НОВАЯ, НАДЕЖНАЯ ФУНКЦИЯ РУЧНОГО РАЗБОРА ---\n",
    "def brute_force_split(word, ahocs):\n",
    "    \"\"\"\n",
    "    Пытается \"взломать\" слово, которое сама библиотека считает атомарным.\n",
    "    Ищет первую же комбинацию из двух частей, которые есть в словаре.\n",
    "    \"\"\"\n",
    "    # Минимальная длина части слова, чтобы избежать мусора вроде \"s\", \"en\"\n",
    "    MIN_PART_LENGTH = 3\n",
    "    \n",
    "    # Идем от самой длинной возможной первой части к самой короткой\n",
    "    for i in range(len(word) - MIN_PART_LENGTH, MIN_PART_LENGTH - 1, -1):\n",
    "        part1 = word[:i]\n",
    "        part2 = word[i:]\n",
    "        \n",
    "        # Проверяем, есть ли обе части в словаре.\n",
    "        # ВАЖНО: Проверяем и с большой, и с маленькой буквы, так как\n",
    "        # части слова могут быть как существительными, так и нет.\n",
    "        if ahocs.exists(part1) and (ahocs.exists(part2) or ahocs.exists(part2.capitalize())):\n",
    "            # Если нашли удачную комбинацию, немедленно возвращаем ее\n",
    "            # и рекурсивно вызываем эту же функцию для КАЖДОЙ из новых частей.\n",
    "            print(f\"  -> '{word}' ==> manually split into: ['{part1}', '{part2}']\")\n",
    "            return brute_force_split(part1, ahocs) + brute_force_split(part2, ahocs)\n",
    "            \n",
    "    # Если после всех попыток слово так и не удалось разбить,\n",
    "    # значит, оно действительно атомарно. Возвращаем его как есть.\n",
    "    print(f\"  -> '{word}' is confirmed atomic.\")\n",
    "    return [word]\n",
    "\n",
    "# --- ОСНОВНАЯ ЧАСТЬ ---\n",
    "\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nERROR: Dictionary file '{dictionary_file}' not found!\")\n",
    "else:\n",
    "    print(f\"\\nDictionary file found: '{dictionary_file}'. Loading...\")\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Dictionary loaded successfully.\")\n",
    "\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "        \n",
    "        # 1. Делаем ПЕРВЫЙ, грубый разбор с помощью библиотеки.\n",
    "        print(\"\\nStep 1: Initial rough split using the library...\")\n",
    "        initial_parts = comp_split.merge_fractions(\n",
    "            comp_split.dissect(compound_word, ahocs, make_singular=True)\n",
    "        )\n",
    "        print(f\"Initial parts: {initial_parts}\")\n",
    "        \n",
    "        # 2. Теперь для КАЖДОЙ части из этого списка вызываем нашу \"упрямую\" функцию.\n",
    "        print(\"\\nStep 2: Deep splitting each part...\")\n",
    "        final_components = []\n",
    "        for part in initial_parts:\n",
    "            # .extend добавляет все элементы из возвращенного списка\n",
    "            final_components.extend(brute_force_split(part, ahocs))\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Analyzing word: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        print(\"✅✅✅ SUCCESS! Final deep split results:\")\n",
    "        for i, part in enumerate(final_components, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a046ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Кардинально переделанный код с новым, надежным алгоритмом ---\n",
    "\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Library 'german-compound-splitter' imported successfully.\")\n",
    "\n",
    "# --- НОВАЯ, ПРОСТАЯ И НАДЕЖНАЯ ФУНКЦИЯ ---\n",
    "def deep_split(compound_word, ahocs):\n",
    "    \"\"\"\n",
    "    Выполняет глубокий, итеративный разбор слова до тех пор,\n",
    "    пока все части не станут атомарными.\n",
    "    \"\"\"\n",
    "    # Создаем \"очередь\" слов, которые нужно проверить. Начинаем с одного слова.\n",
    "    words_to_process = [compound_word]\n",
    "    \n",
    "    # Здесь будут храниться финальные, самые мелкие части.\n",
    "    final_parts = []\n",
    "    \n",
    "    # Цикл будет работать, пока в очереди есть слова для обработки.\n",
    "    while words_to_process:\n",
    "        # Берем первое слово из очереди.\n",
    "        current_word = words_to_process.pop(0)\n",
    "        \n",
    "        # Пытаемся разбить его с помощью библиотеки.\n",
    "        # Это наш основной и единственный инструмент.\n",
    "        parts = comp_split.merge_fractions(\n",
    "            comp_split.dissect(current_word, ahocs, make_singular=True)\n",
    "        )\n",
    "        \n",
    "        # ПРОВЕРКА: Удалось ли разбить слово?\n",
    "        # Если результат - это НЕ одно слово, равное исходному, значит, разбор удался.\n",
    "        if len(parts) > 1 or (len(parts) == 1 and parts[0].lower() != current_word.lower()):\n",
    "            # Если разбор удался, мы добавляем новые, более мелкие части\n",
    "            # В НАЧАЛО очереди, чтобы обработать их следующими.\n",
    "            print(f\"  -> '{current_word}' ==> {parts}. Adding to queue.\")\n",
    "            words_to_process = parts + words_to_process\n",
    "        else:\n",
    "            # Если разбор НЕ удался (слово атомарно),\n",
    "            # мы добавляем его в наш финальный результат.\n",
    "            print(f\"  -> '{current_word}' is atomic. Adding to final list.\")\n",
    "            final_parts.append(current_word)\n",
    "            \n",
    "    return final_parts\n",
    "\n",
    "# --- ОСНОВНАЯ ЧАСТЬ ---\n",
    "\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nERROR: Dictionary file '{dictionary_file}' not found!\")\n",
    "else:\n",
    "    print(f\"\\nDictionary file found: '{dictionary_file}'. Loading...\")\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Dictionary loaded successfully.\")\n",
    "\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "        \n",
    "        print(\"\\nStarting deep split process...\")\n",
    "        final_components = deep_split(compound_word, ahocs)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Analyzing word: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        print(\"✅✅✅ SUCCESS! Deep (iterative) split results:\")\n",
    "        for i, part in enumerate(final_components, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Кардинально переделанный код с новым, надежным и простым алгоритмом ---\n",
    "\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Библиотека 'german-compound-splitter' успешно импортирована.\")\n",
    "\n",
    "# --- НОВАЯ, НАДЕЖНАЯ ФУНКЦИЯ РУЧНОГО РАЗБОРА ---\n",
    "def razdelit_slovo(slovo, ahocs):\n",
    "    \"\"\"\n",
    "    \"Взломывает\" слово, проверяя все возможные точки разрыва.\n",
    "    Находит самую длинную первую часть, которая есть в словаре,\n",
    "    и для которой остаток также есть в словаре.\n",
    "    \"\"\"\n",
    "    # Минимальная длина части, чтобы избежать мусора\n",
    "    MIN_DLINA_CHASTI = 3\n",
    "    \n",
    "    # Идем от самой длинной возможной первой части к самой короткой\n",
    "    for i in range(len(slovo) - MIN_DLINA_CHASTI, MIN_DLINA_CHASTI - 1, -1):\n",
    "        chast1 = slovo[:i]\n",
    "        chast2 = slovo[i:]\n",
    "        \n",
    "        # Проверяем, есть ли обе части в словаре.\n",
    "        # ВАЖНО: Проверяем и с большой, и с маленькой буквы, так как\n",
    "        # части могут быть и существительными, и другими частями речи.\n",
    "        chast1_sushchestvuet = ahocs.exists(chast1) or ahocs.exists(chast1.capitalize())\n",
    "        chast2_sushchestvuet = ahocs.exists(chast2) or ahocs.exists(chast2.capitalize())\n",
    "        \n",
    "        if chast1_sushchestvuet and chast2_sushchestvuet:\n",
    "            # Если нашли удачную комбинацию, немедленно возвращаем ее\n",
    "            return [chast1, chast2]\n",
    "            \n",
    "    # Если после всех попыток слово так и не удалось разбить,\n",
    "    # значит, оно действительно атомарно. Возвращаем его как есть.\n",
    "    return [slovo]\n",
    "\n",
    "# --- ОСНОВНАЯ ЧАСТЬ ---\n",
    "\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nОШИБКА: Файл словаря '{dictionary_file}' не найден!\")\n",
    "else:\n",
    "    print(f\"\\nНайден файл словаря: '{dictionary_file}'. Загружаем...\")\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Словарь успешно загружен.\")\n",
    "\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "        \n",
    "        # Создаем \"очередь\" слов для обработки. Начинаем с одного.\n",
    "        ochered_slov = [compound_word]\n",
    "        finalnye_chasti = []\n",
    "        \n",
    "        print(\"\\nНачинаем процесс глубокого разбора...\")\n",
    "        \n",
    "        # Цикл работает, пока в очереди есть слова.\n",
    "        while ochered_slov:\n",
    "            tekushchee_slovo = ochered_slov.pop(0)\n",
    "            \n",
    "            # Вызываем НАШУ СОБСТВЕННУЮ функцию разбора.\n",
    "            chasti = razdelit_slovo(tekushchee_slovo, ahocs)\n",
    "            \n",
    "            # Если слово было успешно разделено на несколько частей...\n",
    "            if len(chasti) > 1:\n",
    "                print(f\"  -> '{tekushchee_slovo}' ==> {chasti}. Добавляем в очередь.\")\n",
    "                # ...добавляем эти новые части в начало очереди для дальнейшей проверки.\n",
    "                ochered_slov = chasti + ochered_slov\n",
    "            else:\n",
    "                # Если слово не разделилось (оно атомарно)...\n",
    "                print(f\"  -> '{tekushchee_slovo}' атомарно. Добавляем в финальный список.\")\n",
    "                # ...добавляем его в итоговый список.\n",
    "                finalnye_chasti.append(tekushchee_slovo)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Анализируем слово: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        print(\"✅✅✅ ПОБЕДА! Финальный глубокий разбор:\")\n",
    "        for i, part in enumerate(finalnye_chasti, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nПроизошла ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84724c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Финальный, рабочий код, основанный на новой, правильной логике ---\n",
    "\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"Библиотека 'german-compound-splitter' успешно импортирована.\")\n",
    "\n",
    "# --- НОВАЯ ФУНКЦИЯ-\"МОЛОТОК\" ---\n",
    "def razbit_slovarnoe_slovo(slovo, ahocs):\n",
    "    \"\"\"\n",
    "    \"Взламывает\" слово, которое библиотека считает цельным (например, Schifffahrt).\n",
    "    Находит первую же комбинацию из двух частей, которые есть в словаре.\n",
    "    \"\"\"\n",
    "    MIN_DLINA_CHASTI = 3\n",
    "    for i in range(len(slovo) - MIN_DLINA_CHASTI, MIN_DLINA_CHASTI - 1, -1):\n",
    "        chast1 = slovo[:i]\n",
    "        chast2 = slovo[i:]\n",
    "        \n",
    "        # Проверяем, есть ли обе части в словаре.\n",
    "        if ahocs.exists(chast1.capitalize()) and ahocs.exists(chast2.capitalize()):\n",
    "            return [chast1, chast2] # Нашли! Возвращаем результат.\n",
    "            \n",
    "    return [slovo] # Не нашли, слово действительно атомарно.\n",
    "\n",
    "# --- ОСНОВНАЯ ЧАСТЬ ---\n",
    "\n",
    "dictionary_file = 'german.dic'\n",
    "\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"\\nОШИБКА: Файл словаря '{dictionary_file}' не найден!\")\n",
    "else:\n",
    "    print(f\"\\nНайден файл словаря: '{dictionary_file}'. Загружаем...\")\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(\"Словарь успешно загружен.\")\n",
    "\n",
    "        compound_word = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "        \n",
    "        # 1. Используем \"скальпель\" для грубого разбора.\n",
    "        print(\"\\nШаг 1: Первичный разбор с помощью библиотеки...\")\n",
    "        ochered_slov = comp_split.merge_fractions(\n",
    "            comp_split.dissect(compound_word, ahocs, make_singular=True)\n",
    "        )\n",
    "        print(f\"  -> Первичные части: {ochered_slov}\")\n",
    "        \n",
    "        finalnye_chasti = []\n",
    "        \n",
    "        # Цикл работает, пока в очереди есть что-то для обработки.\n",
    "        while ochered_slov:\n",
    "            tekushchee_slovo = ochered_slov.pop(0)\n",
    "            \n",
    "            # 2. Используем \"молоток\", чтобы разбить каждую часть дальше.\n",
    "            chasti = razbit_slovarnoe_slovo(tekushchee_slovo, ahocs)\n",
    "            \n",
    "            if len(chasti) > 1:\n",
    "                # Если \"молоток\" разбил слово, добавляем новые части в начало очереди.\n",
    "                print(f\"  -> '{tekushchee_slovo}' ==> разбит на {chasti}. Возвращаем в очередь.\")\n",
    "                ochered_slov = chasti + ochered_slov\n",
    "            else:\n",
    "                # Если даже \"молоток\" не смог разбить слово, оно точно атомарно.\n",
    "                print(f\"  -> '{tekushchee_slovo}' подтвержден как атомарный. В результат.\")\n",
    "                finalnye_chasti.append(tekushchee_slovo)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"Анализируем слово: '{compound_word}'\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        print(\"✅✅✅ ПОБЕДА! Финальный глубокий разбор:\")\n",
    "        for i, part in enumerate(finalnye_chasti, 1):\n",
    "            print(f\"  {i}. {part}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nПроизошла ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84cf560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ШАГ 1: Загрузка моделей и словарей ---\n",
      "✅ Модель SpaCy 'de_core_news_lg' успешно загружена.\n",
      "Loading data file - german.dic\n",
      "✅ Словарь 'german.dic' успешно загружен.\n",
      "\n",
      "==================================================\n",
      "--- ШАГ 2: Тестирование лемматизации в SpaCy ---\n",
      "==================================================\n",
      "Исходное слово: 'Ananaserdbeeren'\n",
      "Лемма по версии SpaCy: 'Ananaserdbeeren'\n",
      "❗️ РЕЗУЛЬТАТ: SpaCy НЕ СМОГ найти лемму и вернул исходное слово.\n",
      "\n",
      "==================================================\n",
      "--- ШАГ 3: Тестирование сингуляризации в GCS ---\n",
      "==================================================\n",
      "Dissect compound:  Ananaserdbeeren\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m dissection \u001b[38;5;241m=\u001b[39m comp_split\u001b[38;5;241m.\u001b[39mdissect(problem_word, ahocs, make_singular\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Попробуем собрать слово обратно из частей, чтобы увидеть результат сингуляризации\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m reconstructed_singular \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d\u001b[38;5;241m.\u001b[39mword \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dissection])\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mИсходное слово: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblem_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mРезультат сингуляризации через GCS: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreconstructed_singular\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 59\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m dissection \u001b[38;5;241m=\u001b[39m comp_split\u001b[38;5;241m.\u001b[39mdissect(problem_word, ahocs, make_singular\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Попробуем собрать слово обратно из частей, чтобы увидеть результат сингуляризации\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m reconstructed_singular \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dissection])\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mИсходное слово: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblem_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mРезультат сингуляризации через GCS: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreconstructed_singular\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'word'"
     ]
    }
   ],
   "source": [
    "# 20250826181301\n",
    "# --- Изолированный тест для \"Ananaserdbeeren\" ---\n",
    "\n",
    "# 1. Импорты\n",
    "import spacy\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"--- ШАГ 1: Загрузка моделей и словарей ---\")\n",
    "\n",
    "# 2. Загружаем большую модель SpaCy для немецкого\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "    print(\"✅ Модель SpaCy 'de_core_news_lg' успешно загружена.\")\n",
    "except OSError:\n",
    "    print(\"❌ ОШИБКА: Модель 'de_core_news_lg' не найдена.\")\n",
    "    print(\"   Пожалуйста, установите ее, выполнив в терминале: python -m spacy download de_core_news_lg\")\n",
    "    nlp = None\n",
    "\n",
    "# 3. Загружаем словарь для german-compound-splitter\n",
    "dictionary_file = 'german.dic'\n",
    "ahocs = None\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"❌ ОШИБКА: Файл словаря '{dictionary_file}' не найден!\")\n",
    "else:\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(f\"✅ Словарь '{dictionary_file}' успешно загружен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ОШИБКА при загрузке словаря: {e}\")\n",
    "\n",
    "# 4. Определяем наше проблемное слово\n",
    "problem_word = \"Ananaserdbeeren\"\n",
    "\n",
    "# 5. Проводим тесты, если все загрузилось\n",
    "if nlp and ahocs:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- ШАГ 2: Тестирование лемматизации в SpaCy ---\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    doc = nlp(problem_word)\n",
    "    token = doc[0]\n",
    "    \n",
    "    print(f\"Исходное слово: '{token.text}'\")\n",
    "    print(f\"Лемма по версии SpaCy: '{token.lemma_}'\")\n",
    "    if token.text == token.lemma_:\n",
    "        print(\"❗️ РЕЗУЛЬТАТ: SpaCy НЕ СМОГ найти лемму и вернул исходное слово.\")\n",
    "    else:\n",
    "        print(\"✅ РЕЗУЛЬТАТ: SpaCy нашел лемму.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- ШАГ 3: Тестирование сингуляризации в GCS ---\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Это та логика, которую я пытался использовать для исправления\n",
    "    dissection = comp_split.dissect(problem_word, ahocs, make_singular=True)\n",
    "    \n",
    "    # Попробуем собрать слово обратно из частей, чтобы увидеть результат сингуляризации\n",
    "    reconstructed_singular = \"\".join([d.word for d in dissection])\n",
    "\n",
    "    print(f\"Исходное слово: '{problem_word}'\")\n",
    "    print(f\"Результат сингуляризации через GCS: '{reconstructed_singular}'\")\n",
    "    \n",
    "    if problem_word == reconstructed_singular:\n",
    "        print(\"❗️ РЕЗУЛЬТАТ: GCS (с опцией make_singular) ТАКЖЕ НЕ СМОГ найти форму ед.ч.\")\n",
    "    else:\n",
    "        print(\"✅ РЕЗУЛЬТАТ: GCS нашел форму ед.ч.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd1a3f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ШАГ 1: Загрузка моделей и словарей ---\n",
      "✅ Модель SpaCy 'de_core_news_lg' успешно загружена.\n",
      "Loading data file - german.dic\n",
      "✅ Словарь 'german.dic' успешно загружен.\n",
      "\n",
      "==================================================\n",
      "--- ШАГ 2: Тестирование лемматизации в SpaCy ---\n",
      "==================================================\n",
      "Исходное слово: 'Ananaserdbeeren'\n",
      "Лемма по версии SpaCy: 'Ananaserdbeeren'\n",
      "❗️ РЕЗУЛЬТАТ: SpaCy НЕ СМОГ найти лемму и вернул исходное слово.\n",
      "\n",
      "==================================================\n",
      "--- ШАГ 3: Тестирование сингуляризации в GCS ---\n",
      "==================================================\n",
      "Dissect compound:  Ananaserdbeeren\n",
      "Исходное слово: 'Ananaserdbeeren'\n",
      "Компоненты от GCS: ['Ananas', 'Erdbeere']\n",
      "Собранное обратно слово: 'AnanasErdbeere'\n",
      "✅ РЕЗУЛЬТАТ: GCS нашел форму ед.ч.\n"
     ]
    }
   ],
   "source": [
    "# --- Изолированный тест для \"Ananaserdbeeren\" (Исправленная версия) ---\n",
    "\n",
    "# 1. Импорты\n",
    "import spacy\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"--- ШАГ 1: Загрузка моделей и словарей ---\")\n",
    "\n",
    "# 2. Загружаем большую модель SpaCy для немецкого\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "    print(\"✅ Модель SpaCy 'de_core_news_lg' успешно загружена.\")\n",
    "except OSError:\n",
    "    print(\"❌ ОШИБКА: Модель 'de_core_news_lg' не найдена.\")\n",
    "    print(\"   Пожалуйста, установите ее, выполнив в терминале: python -m spacy download de_core_news_lg\")\n",
    "    nlp = None\n",
    "\n",
    "# 3. Загружаем словарь для german-compound-splitter\n",
    "dictionary_file = 'german.dic'\n",
    "ahocs = None\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"❌ ОШИБКА: Файл словаря '{dictionary_file}' не найден!\")\n",
    "else:\n",
    "    try:\n",
    "        ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(f\"✅ Словарь '{dictionary_file}' успешно загружен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ОШИБКА при загрузке словаря: {e}\")\n",
    "\n",
    "# 4. Определяем наше проблемное слово\n",
    "problem_word = \"Ananaserdbeeren\"\n",
    "\n",
    "# 5. Проводим тесты, если все загрузилось\n",
    "if nlp and ahocs:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- ШАГ 2: Тестирование лемматизации в SpaCy ---\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    doc = nlp(problem_word)\n",
    "    token = doc[0]\n",
    "    \n",
    "    print(f\"Исходное слово: '{token.text}'\")\n",
    "    print(f\"Лемма по версии SpaCy: '{token.lemma_}'\")\n",
    "    if token.text == token.lemma_:\n",
    "        print(\"❗️ РЕЗУЛЬТАТ: SpaCy НЕ СМОГ найти лемму и вернул исходное слово.\")\n",
    "    else:\n",
    "        print(\"✅ РЕЗУЛЬТАТ: SpaCy нашел лемму.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- ШАГ 3: Тестирование сингуляризации в GCS ---\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # *** ИСПРАВЛЕННАЯ СТРОКА ЗДЕСЬ ***\n",
    "    # comp_split.dissect возвращает кортеж из двух списков строк.\n",
    "    # Нас интересует второй список, который содержит компоненты.\n",
    "    # merge_fractions объединяет их в финальный список строк.\n",
    "    dissection_result = comp_split.dissect(problem_word, ahocs, make_singular=True)\n",
    "    final_components = comp_split.merge_fractions(dissection_result)\n",
    "    \n",
    "    # Собираем слово обратно из компонентов, чтобы увидеть результат\n",
    "    reconstructed_singular = \"\".join(final_components)\n",
    "\n",
    "    print(f\"Исходное слово: '{problem_word}'\")\n",
    "    print(f\"Компоненты от GCS: {final_components}\")\n",
    "    print(f\"Собранное обратно слово: '{reconstructed_singular}'\")\n",
    "    \n",
    "    if problem_word == reconstructed_singular:\n",
    "        print(\"❗️ РЕЗУЛЬТАТ: GCS (с опцией make_singular) ТАКЖЕ НЕ СМОГ найти форму ед.ч.\")\n",
    "    else:\n",
    "        print(\"✅ РЕЗУЛЬТАТ: GCS нашел форму ед.ч.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15878e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ШАГ 1: Загрузка словаря ---\n",
      "✅ Словарь 'german.dic' успешно загружен.\n",
      "\n",
      "============================================================\n",
      "--- ТЕСТ 1: Длинное слово во множественном числе ---\n",
      "============================================================\n",
      "Исходное слово: 'Donaudampfschifffahrtsgesellschaftskapitäne'\n",
      "Dissect compound:  Donaudampfschifffahrtsgesellschaftskapitäne\n",
      "Компоненты от GCS: ['Donau', 'Dampf', 'Schifffahrt', 'Gesellschaft', 'Kapitän']\n",
      "Последний компонент: 'Kapitän'\n",
      "✅ РЕЗУЛЬТАТ: Умляут (ä) сохранен.\n",
      "\n",
      "============================================================\n",
      "--- ТЕСТ 2: Короткое слово 'Kapitäne' (контрольный тест) ---\n",
      "============================================================\n",
      "Исходное слово: 'Kapitäne'\n",
      "Dissect compound:  Kapitäne\n",
      "Компоненты от GCS: ['Kapitän']\n",
      "Результат сингуляризации: 'Kapitän'\n",
      "✅ РЕЗУЛЬТАТ: 'Kapitäne' -> 'Kapitän'. Корректно.\n"
     ]
    }
   ],
   "source": [
    "# 20250826183344\n",
    "# --- Изолированный тест для \"Donaudampfschifffahrtsgesellschaftskapitäne\" ---\n",
    "\n",
    "# 1. Импорты\n",
    "from german_compound_splitter import comp_split\n",
    "import os\n",
    "\n",
    "print(\"--- ШАГ 1: Загрузка словаря ---\")\n",
    "\n",
    "# 2. Загружаем словарь\n",
    "dictionary_file = 'german.dic'\n",
    "ahocs = None\n",
    "if not os.path.exists(dictionary_file):\n",
    "    print(f\"❌ ОШИБКА: Файл словаря '{dictionary_file}' не найден!\")\n",
    "else:\n",
    "    try:\n",
    "        # Подавляем стандартный вывод \"Loading data file...\"\n",
    "        # чтобы не мешать нашему выводу\n",
    "        from contextlib import redirect_stdout\n",
    "        import io\n",
    "        with redirect_stdout(io.StringIO()):\n",
    "            ahocs = comp_split.read_dictionary_from_file(dictionary_file)\n",
    "        print(f\"✅ Словарь '{dictionary_file}' успешно загружен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ОШИБКА при загрузке словаря: {e}\")\n",
    "\n",
    "# 3. Слова для тестирования\n",
    "long_word_plural = \"Donaudampfschifffahrtsgesellschaftskapitäne\"\n",
    "short_word_plural = \"Kapitäne\"\n",
    "\n",
    "# 4. Проводим тесты, если словарь загрузился\n",
    "if ahocs:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"--- ТЕСТ 1: Длинное слово во множественном числе ---\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Исходное слово: '{long_word_plural}'\")\n",
    "    \n",
    "    # Вызываем GCS с флагом make_singular=True\n",
    "    dissection_result = comp_split.dissect(long_word_plural, ahocs, make_singular=True)\n",
    "    final_components = comp_split.merge_fractions(dissection_result)\n",
    "    \n",
    "    print(f\"Компоненты от GCS: {final_components}\")\n",
    "    \n",
    "    last_component = final_components[-1] if final_components else \"\"\n",
    "    print(f\"Последний компонент: '{last_component}'\")\n",
    "    \n",
    "    if \"ä\" in last_component:\n",
    "        print(\"✅ РЕЗУЛЬТАТ: Умляут (ä) сохранен.\")\n",
    "    else:\n",
    "        print(\"❗️ РЕЗУЛЬТАТ: Умляут (ä) был потерян и заменен на 'a'.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"--- ТЕСТ 2: Короткое слово 'Kapitäne' (контрольный тест) ---\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Исходное слово: '{short_word_plural}'\")\n",
    "    \n",
    "    # Вызываем GCS с флагом make_singular=True\n",
    "    dissection_result_short = comp_split.dissect(short_word_plural, ahocs, make_singular=True)\n",
    "    final_components_short = comp_split.merge_fractions(dissection_result_short)\n",
    "    \n",
    "    reconstructed_singular = \"\".join(final_components_short)\n",
    "    \n",
    "    print(f\"Компоненты от GCS: {final_components_short}\")\n",
    "    print(f\"Результат сингуляризации: '{reconstructed_singular}'\")\n",
    "\n",
    "    if reconstructed_singular == \"Kapitän\":\n",
    "        print(\"✅ РЕЗУЛЬТАТ: 'Kapitäne' -> 'Kapitän'. Корректно.\")\n",
    "    elif reconstructed_singular == \"Kapitan\":\n",
    "        print(\"❗️ РЕЗУЛЬТАТ: 'Kapitäne' -> 'Kapitan'. Умляут потерян.\")\n",
    "    else:\n",
    "        print(f\"❓ РЕЗУЛЬТАТ: Неожиданный результат - '{reconstructed_singular}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a28b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ШАГ 1: Загрузка языковой модели SpaCy ---\n",
      "✅ Модель 'de_core_news_lg' успешно загружена.\n",
      "\n",
      "--- ШАГ 2: Анализ слова ---\n",
      "\n",
      "Исходное слово: 'Donaudampfschifffahrtsgesellschaftskapitäne'\n",
      "Лемма от SpaCy:  'Donaudampfschifffahrtsgesellschaftskapitan'\n",
      "\n",
      "--- ШАГ 3: ВЫВОДЫ ---\n",
      "❗️ ВЕРДИКТ: SpaCy ошибся и ПОТЕРЯЛ умляут. Проблема подтверждена.\n"
     ]
    }
   ],
   "source": [
    "# --- Изолированный тест для проверки лемматизации SpaCy ---\n",
    "\n",
    "# 1. Импортируем библиотеку\n",
    "import spacy\n",
    "\n",
    "print(\"--- ШАГ 1: Загрузка языковой модели SpaCy ---\")\n",
    "\n",
    "# 2. Загружаем большую модель для немецкого языка.\n",
    "#    Она дает самые точные результаты.\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "    print(\"✅ Модель 'de_core_news_lg' успешно загружена.\")\n",
    "except OSError:\n",
    "    print(\"❌ ОШИбка: Модель 'de_core_news_lg' не найдена.\")\n",
    "    print(\"   Пожалуйста, установите ее, выполнив в терминале (или в ячейке ноутбука с '!'):\")\n",
    "    print(\"   !python -m spacy download de_core_news_lg\")\n",
    "    # Если модель не установлена, дальнейшее выполнение бессмысленно\n",
    "    nlp = None\n",
    "\n",
    "# 3. Проводим тест, только если модель загрузилась\n",
    "if nlp:\n",
    "    print(\"\\n--- ШАГ 2: Анализ слова ---\")\n",
    "    \n",
    "    # Наше слово для проверки\n",
    "    word_to_test = \"Donaudampfschifffahrtsgesellschaftskapitäne\"\n",
    "    \n",
    "    # Обрабатываем текст с помощью SpaCy\n",
    "    doc = nlp(word_to_test)\n",
    "    \n",
    "    # Так как у нас всего одно слово, получаем первый (и единственный) токен\n",
    "    token = doc[0]\n",
    "    \n",
    "    # Получаем лемму этого токена\n",
    "    lemma = token.lemma_\n",
    "    \n",
    "    print(f\"\\nИсходное слово: '{token.text}'\")\n",
    "    print(f\"Лемма от SpaCy:  '{lemma}'\")\n",
    "    \n",
    "    print(\"\\n--- ШАГ 3: ВЫВОДЫ ---\")\n",
    "    \n",
    "    correct_lemma = \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
    "    incorrect_lemma = \"Donaudampfschifffahrtsgesellschaftskapitan\"\n",
    "    \n",
    "    if lemma == correct_lemma:\n",
    "        print(\"✅ ВЕРДИКТ: SpaCy отработал корректно и СОХРАНИЛ умляут.\")\n",
    "    elif lemma == incorrect_lemma:\n",
    "        print(\"❗️ ВЕРДИКТ: SpaCy ошибся и ПОТЕРЯЛ умляут. Проблема подтверждена.\")\n",
    "    else:\n",
    "        print(f\"❓ ВЕРДИКТ: SpaCy вернул неожиданный результат, который не совпадает ни с одним из ожидаемых.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0eb333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ШАГ 1: Загрузка языковой модели SpaCy ---\n",
      "✅ Модель 'de_core_news_lg' успешно загружена.\n",
      "\n",
      "--- ШАГ 2: Анализ слова ---\n",
      "\n",
      "Исходное слово: 'Donaudampfschifffahrtsgesellschaftskapitäns'\n",
      "Лемма от SpaCy:  'Donaudampfschifffahrtsgesellschaftskapitäns'\n",
      "\n",
      "--- ШАГ 3: ВЫВОДЫ ---\n",
      "❗️ ВЕРДИКТ: SpaCy не смог найти лемму и вернул исходное слово. Проблема подтверждена.\n"
     ]
    }
   ],
   "source": [
    "# --- Изолированный тест для проверки лемматизации SpaCy (форма Genitiv) ---\n",
    "\n",
    "# 1. Импортируем библиотеку\n",
    "import spacy\n",
    "\n",
    "print(\"--- ШАГ 1: Загрузка языковой модели SpaCy ---\")\n",
    "\n",
    "# 2. Загружаем большую модель для немецкого языка.\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "    print(\"✅ Модель 'de_core_news_lg' успешно загружена.\")\n",
    "except OSError:\n",
    "    print(\"❌ ОШИБКА: Модель 'de_core_news_lg' не найдена.\")\n",
    "    print(\"   Пожалуйста, установите ее, выполнив в терминале (или в ячейке ноутбука с '!'):\")\n",
    "    print(\"   !python -m spacy download de_core_news_lg\")\n",
    "    nlp = None\n",
    "\n",
    "# 3. Проводим тест, только если модель загрузилась\n",
    "if nlp:\n",
    "    print(\"\\n--- ШАГ 2: Анализ слова ---\")\n",
    "    \n",
    "    # Новое слово для проверки (генитив, единственное число)\n",
    "    word_to_test = \"Donaudampfschifffahrtsgesellschaftskapitäns\"\n",
    "    \n",
    "    # Обрабатываем текст с помощью SpaCy\n",
    "    doc = nlp(word_to_test)\n",
    "    token = doc[0]\n",
    "    \n",
    "    # Получаем лемму этого токена\n",
    "    lemma = token.lemma_\n",
    "    \n",
    "    print(f\"\\nИсходное слово: '{token.text}'\")\n",
    "    print(f\"Лемма от SpaCy:  '{lemma}'\")\n",
    "    \n",
    "    print(\"\\n--- ШАГ 3: ВЫВОДЫ ---\")\n",
    "    \n",
    "    # Ожидаемая правильная лемма (номинатив, единственное число)\n",
    "    correct_lemma = \"Kapitän\"\n",
    "    \n",
    "    if lemma == correct_lemma:\n",
    "        print(\"✅ ВЕРДИКТ: SpaCy отработал корректно и нашел правильную базовую лемму (удалил окончание генитива '-s').\")\n",
    "    elif lemma == word_to_test:\n",
    "        print(\"❗️ ВЕРДИКТ: SpaCy не смог найти лемму и вернул исходное слово. Проблема подтверждена.\")\n",
    "    else:\n",
    "        print(f\"❓ ВЕРДИКТ: SpaCy вернул неожиданный результат ('{lemma}'), который не является ни исходным словом, ни правильной леммой.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0dc2b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ШАГ 1: Загрузка языковой модели SpaCy ---\n",
      "✅ Модель 'de_core_news_lg' успешно загружена.\n",
      "\n",
      "--- ШАГ 2: Анализ слова ---\n",
      "\n",
      "Исходное слово: 'Kapitäns'\n",
      "Лемма от SpaCy:  'Kapitän'\n",
      "\n",
      "--- ШАГ 3: ВЫВОДЫ ---\n",
      "✅ ВЕРДИКТ: SpaCy отработал корректно и нашел правильную базовую лемму (удалил окончание генитива '-s').\n"
     ]
    }
   ],
   "source": [
    "# --- Изолированный тест для проверки лемматизации SpaCy (форма Genitiv) ---\n",
    "\n",
    "# 1. Импортируем библиотеку\n",
    "import spacy\n",
    "\n",
    "print(\"--- ШАГ 1: Загрузка языковой модели SpaCy ---\")\n",
    "\n",
    "# 2. Загружаем большую модель для немецкого языка.\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "    print(\"✅ Модель 'de_core_news_lg' успешно загружена.\")\n",
    "except OSError:\n",
    "    print(\"❌ ОШИБКА: Модель 'de_core_news_lg' не найдена.\")\n",
    "    print(\"   Пожалуйста, установите ее, выполнив в терминале (или в ячейке ноутбука с '!'):\")\n",
    "    print(\"   !python -m spacy download de_core_news_lg\")\n",
    "    nlp = None\n",
    "\n",
    "# 3. Проводим тест, только если модель загрузилась\n",
    "if nlp:\n",
    "    print(\"\\n--- ШАГ 2: Анализ слова ---\")\n",
    "    \n",
    "    # Новое слово для проверки (генитив, единственное число)\n",
    "    word_to_test = \"Kapitäns\"\n",
    "    \n",
    "    # Обрабатываем текст с помощью SpaCy\n",
    "    doc = nlp(word_to_test)\n",
    "    token = doc[0]\n",
    "    \n",
    "    # Получаем лемму этого токена\n",
    "    lemma = token.lemma_\n",
    "    \n",
    "    print(f\"\\nИсходное слово: '{token.text}'\")\n",
    "    print(f\"Лемма от SpaCy:  '{lemma}'\")\n",
    "    \n",
    "    print(\"\\n--- ШАГ 3: ВЫВОДЫ ---\")\n",
    "    \n",
    "    # Ожидаемая правильная лемма (номинатив, единственное число)\n",
    "    correct_lemma = \"Kapitän\"\n",
    "    \n",
    "    if lemma == correct_lemma:\n",
    "        print(\"✅ ВЕРДИКТ: SpaCy отработал корректно и нашел правильную базовую лемму (удалил окончание генитива '-s').\")\n",
    "    elif lemma == word_to_test:\n",
    "        print(\"❗️ ВЕРДИКТ: SpaCy не смог найти лемму и вернул исходное слово. Проблема подтверждена.\")\n",
    "    else:\n",
    "        print(f\"❓ ВЕРДИКТ: SpaCy вернул неожиданный результат ('{lemma}'), который не является ни исходным словом, ни правильной леммой.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20250825231214-spacy-env (3.9.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
